# -*- coding: utf-8 -*-
"""Proyek Akhir : Image Classification Model Deployment - Belajar Pengembangan Machine Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/113VlRCfPDdg2BOILeAUWtCrWW_lmIyf0

#Belajar Pengembangan Machine Learning<br>
#Proyek Akhir : Image Classification Model Deployment<br>
Nama  : Yagy Christoper Sumule<br>
Email : yagybatman@gmail.com<br>
Sumber data : https://www.kaggle.com/iarunava/cell-images-for-detecting-malaria

Kriteria :
* Dataset yang akan dipakai bebas, namun minimal memiliki 1000 buah gambar.
* Dataset tidak pernah digunakan pada submission kelas machine learning sebelumnya.
* Dataset dibagi menjadi 80% train set dan 20% test set.
* Model harus menggunakan model sequential.
* Model harus menggunakan Conv2D Maxpooling Layer.
* Akurasi pada training dan validation set minimal sebesar 80%.
* Menggunakan Callback.
* Membuat plot terhadap akurasi dan loss model.
* Menulis kode untuk menyimpan model ke dalam format TF-Lite.

Anda dapat menerapkan beberapa saran untuk mendapatkan nilai tinggi, berikut sarannya:
* Dataset yang digunakan berisi lebih dari 2000 gambar.
* Mengimplementasikan Callback.
* Gambar-gambar pada dataset memiliki resolusi yang tidak seragam.

Detail penilaian submission:
* Bintang 4 : Semua ketentuan terpenuhi, dataset memiliki minimal 2000 sampel gambar dan minimal 3 kelas. Serta akurasi pada training dan validation set minimal 85%.
* Bintang 5 : Semua ketentuan terpenuhi, dataset memiliki minimal 10000 gambar, resolusi gambar pada dataset tidak seragam. Serta akurasi pada training set dan validation set minimal 92%.

## Menyiapkan Library
"""

# Commented out IPython magic to ensure Python compatibility.
import os, zipfile, shutil, PIL
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense
from tensorflow.keras.utils import plot_model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import RMSprop
from tensorflow import keras
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import pathlib
# %matplotlib inline
from google.colab import files

"""##Install Kaggle"""

!pip install -q kaggle

"""## Upload Token API Kaggle"""

uploaded = files.upload()

"""## Konfigurasi untuk menerima datasets dari Kaggle"""

!chmod 600 /content/kaggle.json

"""## Download Dataset"""

! KAGGLE_CONFIG_DIR=/content/ kaggle datasets download -d iarunava/cell-images-for-detecting-malaria

"""## Ekstrak Dataset"""

local_zip = '/content/cell-images-for-detecting-malaria.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content')
zip_ref.close()

"""##Deklarasikan Direktori Dasar"""

base_dir = '/content/cell_images/'

"""## Membuat fungsi list_files untuk mengidentifikasi jumlah file"""

def list_files(startpath):
  num_files = 0
  for root, dirs, files in os.walk(startpath):
    level = root.replace(startpath, '').count(os.sep)
    indent = ' ' * 2 * (level)
    num_files += len(files)
    print('{}{}/ {}'.format(indent, os.path.basename(root), (str(len(files)) + ' images' if len(files) > 0 else '')))
  return num_files

"""##Memanggil fungsi list_files dengan parameter variabel direktori dasar yang telah dibuat sebelumnya"""

list_files(base_dir)

"""##Membuat fungsi read_files untuk membaca setiap files"""

def read_files(startpath):
  image_files = []
  for dirname, dirnames, filenames in os.walk(startpath):
    for filename in filenames:
      image_files.append(os.path.join(dirname, filename))
  return image_files

"""##Menghapus file yang tidak digunakan"""

os.remove("/content/cell_images/Parasitized/Thumbs.db")
os.remove("/content/cell_images/Uninfected/Thumbs.db")
os.remove("/content/cell_images/cell_images/Parasitized/Thumbs.db")
os.remove("/content/cell_images/cell_images/Uninfected/Thumbs.db")

"""##Memastikan ukuran image yang beragam dengan fungsi PIL"""

full_dirs = read_files(base_dir + "cell_images")
image_sizes = []
for file in full_dirs:
  image = PIL.Image.open(file)
  width, height = image.size
  image_sizes.append(f'{width}x{height}')

unique_sizes = set(image_sizes)

print(f'Size all images: {len(image_sizes)}')
print(f'Size unique images: {len(unique_sizes)}')
print(f'First 10 unique images: \n{list(unique_sizes)[:10]}')

"""##Preprocessing Data"""

train_datagen = ImageDataGenerator(
  rescale=1./255,
  validation_split=0.2,
  zoom_range=0.2,
  shear_range=0.2,
  rotation_range=0.2
)

training_generator = train_datagen.flow_from_directory(
  base_dir + "cell_images",
  subset='training',
  target_size=(120,120),
  seed=42,
  batch_size=64,
  interpolation='nearest',
  class_mode='binary',
  classes=['Parasitized','Uninfected']
)

validation_generator = train_datagen.flow_from_directory(
  base_dir + "cell_images",
  subset='validation',
  target_size=(120,120),
  seed=42,
  batch_size=64,
  interpolation='nearest',
  class_mode='binary',
  classes=['Parasitized','Uninfected']
)

"""##Membuat Model"""

model = Sequential([
    Conv2D(64, (3,3), activation='relu', input_shape=(120, 120, 3)),
    MaxPooling2D(2, 2),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Dropout(0.6),
    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Conv2D(256, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Dropout(0.4),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

model.summary()

plot_model(
    model,
    show_shapes=True,
    show_layer_names=True,
)

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if((logs.get('accuracy') > 0.92) and (logs.get('val_accuracy') > 0.92)):
      self.model.stop_training = True
      print("\nAccuracy on training set and validation set has reached > 92%!")
callbacks = myCallback()

LR = 1e-4
num_epochs = 30
model.compile(loss='binary_crossentropy',
              optimizer=RMSprop(learning_rate=LR),
              metrics=['accuracy'])

"""##Melatih Model"""

H = model.fit(
    training_generator,
    validation_data=validation_generator,
    epochs=num_epochs,
    steps_per_epoch=150,
    validation_steps=35,
    callbacks=[callbacks],
    verbose=1
)

"""##Plot Loss"""

loss = H.history["loss"]
val_loss = H.history["val_loss"]
acc = H.history["accuracy"]
val_acc = H.history["val_accuracy"]

plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
plt.plot(loss, label="Training set")
plt.plot(val_loss, label="Validation set", linestyle="--")
plt.title("Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(linestyle="--", linewidth=1, alpha=0.5)

"""#Plot Accuracy"""

plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 2)
plt.plot(acc, label="Training set")
plt.plot(val_acc, label="Validation set", linestyle="--")
plt.title("Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(linestyle="--", linewidth=1, alpha=0.5)

"""##Menyimpan model dalam format SavedModel"""

export_dir = 'saved_model/'
tf.saved_model.save(model, export_dir)

"""##Convert SavedModel menjadi malaria.tflite"""

converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
tflite_model = converter.convert()

tflite_model_file = pathlib.Path('malaria_v1.tflite')
tflite_model_file.write_bytes(tflite_model)